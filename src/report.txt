Experiment 1 (Flowers baseline)

Hypothesis
Training on color photos will score highest on the color validation set, while a true grayscale
model will stay competitive on both color and black-and-white inputs because it never depends on
extra color channels.

Steps
- Loaded the Five Flowers dataset (daisy, dandelion, rose, sunflower, tulip).
- Ran the pretrained ResNet18 without any extra training so we could see a true baseline.
  Used [src/hypertuning.py](src/hypertuning.py) to flip between color and grayscale views.
- Changed the first layer of the network so it can read a single black-and-white channel.
  The code lives in [src/hypertuning.py](src/hypertuning.py#L70-L88).
- Turned training back on and let only the final layer learn for 30 short training loops (epochs).
  The Trainer lives in [mltrainer/trainer.py](.venv/lib/python3.14/site-packages/mltrainer/trainer.py).

Findings (2026-01-02)
- With no extra training the model sits around 15% accuracy, which is basically guessing.
- Simply swapping the first layer to a single channel bumps the grayscale accuracy to about 25%.
- Training the color model for 30 epochs jumps to 87% accuracy on color photos and 69% when we feed it grayscale photos.
- Training the true grayscale model for 30 epochs lands at 82% accuracy on black-and-white photos.
- When we reload the grayscale checkpoint and test it again the color score matches the grayscale score.
  That happens because the test code converts color images to one channel before sending them to the grayscale model.
  The important number is therefore the grayscale accuracy (~80%).

Key notes
- Keep fine-tuning enabled if you add ideas like dropout or unfreezing more layers.
- Give each run a different log folder inside [src/hypertuning.py](src/hypertuning.py) so you do not mix results.
- The grayscale tweak alone gives a quick accuracy bump without any learning.
- Full training on grayscale keeps most of the color performance while dropping two image channels.
- Saving checkpoints (for example [models/flowers_resnet18_gray.pt](models/flowers_resnet18_gray.pt)) lets us reload models quickly.
- The grayscale checkpoint still handles color inputs well enough for mixed-image batches.
- Training speed barely changes because most work still happens inside the frozen ResNet body.

Experiment 2 (3-epoch sweep, 15×color + 15×grayscale)

Goal
Run very short training jobs with lots of different settings to see which ones look promising.

Steps
- Added a sweep mode to [src/hypertuning.py](src/hypertuning.py) that samples learning rate, momentum, and weight decay.
  Each run logs under [Les1_modellogs/hyper_sweep_batch1](Les1_modellogs/hyper_sweep_batch1) and also writes to the CSV at
  [Les1_modellogs/hyper_sweep_batch1/sweep_results.csv](Les1_modellogs/hyper_sweep_batch1/sweep_results.csv).
- Every trial lasts three epochs and we grade it on both the color-style and grayscale-style validation views.

Findings (2026-01-02)
- Color-mode trials average 78% accuracy on full-color validation and 66% when we force those models to view grayscale.
  The best color run (trial 15) hits 90% on color and 78% on grayscale with a learning rate near 0.0039.
- Grayscale-mode trials average 74% accuracy no matter which view we use.
  The best grayscale run (trial 5) scores about 82% with a learning rate near 0.0026 and momentum near 0.55.
- Trial 8 is the best example of a grayscale model that also handles color well (82% vs 81%).
  That one uses a faster learning rate near 0.023.

Next steps
- Use the CSV to pick a handful of hyperparameter sets for deeper training.
- Focus future sweeps on learning rates between 0.002 and 0.01, momentum above 0.8, and small weight decay, because those ranges showed up in the best runs.

Experiment 3 (100-epoch fine-tunes with early stopping)

Goal
Train the top color and grayscale setups for longer, but stop early if the validation score stops improving.

Steps
- Reused the best color sweep run (trial 15) and the best grayscale sweep run (trial 5).
  Trained each for up to 100 epochs with patience set to 10 epochs and a minimum improvement (delta) of 0.001.
  Logs live in [Les1_modellogs/selected/color_trial15_long](Les1_modellogs/selected/color_trial15_long) and
  [Les1_modellogs/selected/grayscale_trial5_long](Les1_modellogs/selected/grayscale_trial5_long).
- Saved the resulting checkpoints at [models/flowers_resnet18_color_trial15_long.pt](models/flowers_resnet18_color_trial15_long.pt)
  and [models/flowers_resnet18_gray_trial5_long.pt](models/flowers_resnet18_gray_trial5_long.pt).
- Reloaded both checkpoints and ran the same evaluation helper so the numbers line up with the earlier experiments.

Findings (2026-01-02)
- Color long-run: reaches 94% accuracy on color validation images and about 79% on grayscale validation images.
  We convert grayscale inputs back to three channels before feeding the color model, so the drop is expected but still acceptable.
- Grayscale long-run: early stopping kicks in at epoch 64 with 88% accuracy on grayscale images.
  The color score matches because we convert color inputs to one channel before the grayscale model sees them.

Next steps
- Decide if one universal grayscale checkpoint is enough or if we should ship both checkpoints for the best results per image type.
- Update any downstream notebooks or scripts to load the new checkpoints and note that grayscale evaluation always happens in single-channel mode.

Experiment 4 (Edge-shift preprocessing test)

Hypothesis
Moving each RGB channel slightly before the grayscale conversion should sharpen edges.
Sharper edges might improve accuracy for models that expect grayscale inputs.

Steps
- Copied a sample flower photo into [data/external/daisy_sample.jpg](data/external/daisy_sample.jpg).
  Generated edge previews at [data/external/daisy_sample_edge_rgb.png](data/external/daisy_sample_edge_rgb.png) and [data/external/daisy_sample_edge_gray.png](data/external/daisy_sample_edge_gray.png).
- Added a channel-shift enhancer (`--edge-shift`) to the CLI so sweeps and single runs can enable the effect.
- Repeated the 3-epoch, 15-trial-per-mode sweep with edge-shift turned on.
  Results live in [Les1_modellogs/hyper_sweep_edge_shift](Les1_modellogs/hyper_sweep_edge_shift) and the CSV at [Les1_modellogs/hyper_sweep_edge_shift/sweep_results.csv](Les1_modellogs/hyper_sweep_edge_shift/sweep_results.csv).

Findings (2026-01-02)
- Color sweeps improved.
  Average color accuracy climbed from 78% to 84%, and the best trial hits 89.6% on color validation and 78.8% on grayscale.
- Grayscale sweeps declined.
  Averages dropped from about 73% to 60%, and the best run leveled off near 70.9% for both validation styles.

Next steps
- Keep the edge-shift flag for color-focused experiments.
  Disable it for grayscale training unless we create a better edge routine.
- Consider rerunning the 30-epoch or 100-epoch color jobs with `--edge-shift` to confirm the short-run gains.

Experiment 5 (Stronger edge shift and deeper heads)

Hypothesis
Pushing the channel shift farther and widening the classifier head should give the color model more edge contrast and capacity, which might recover the old 94% mark or better.

Steps
- Upgraded the CLI in [src/hypertuning.py](src/hypertuning.py) so `--edge-shift-pixels` accepts multiple offsets and the classifier head exposes depth, width, dropout, and unfreeze knobs.
- Generated new previews at [data/external/daisy_sample_edge_shift_px1.png](data/external/daisy_sample_edge_shift_px1.png), [data/external/daisy_sample_edge_shift_px2.png](data/external/daisy_sample_edge_shift_px2.png), and [data/external/daisy_sample_edge_shift_px3.png](data/external/daisy_sample_edge_shift_px3.png) to visualize the stronger shifts.
- Ran a 100-epoch color fine-tune with a 3-pixel shift, 4-layer head, dropout 0.25, width factor 1.6, and one unfrozen residual block. Logs sit in [Les1_modellogs/selected/color_trial15_edge_shift_px3_long](Les1_modellogs/selected/color_trial15_edge_shift_px3_long) with the checkpoint at [models/flowers_resnet18_color_trial15_edge_shift_px3_long.pt](models/flowers_resnet18_color_trial15_edge_shift_px3_long.pt).
- Launched broader 1-epoch color sweeps for shift radii 1 and 2 (50 trials each) under [Les1_modellogs/hyper_sweep_edge_shift_px1](Les1_modellogs/hyper_sweep_edge_shift_px1) and [Les1_modellogs/hyper_sweep_edge_shift_px2](Les1_modellogs/hyper_sweep_edge_shift_px2).

Findings (2026-01-02)
- The px3 long run with patience 12 stopped early at 88.64% validation accuracy (color view) and 65.06% grayscale proxy, so the roomy head has not yet beaten the 94% record.
- Repeating the same setup with patience 15 only nudged the peak to 89.63% color / 66.05% grayscale before early stopping, confirming the stronger edge shift alone is not closing the gap.
- A separate evaluation pass on the saved weights reported 98.86%, but this likely comes from reusing the training transforms mid-stream. We should treat it as a suspiciously high signal until we revalidate via the CLI path.
- The px2 sweep peaks near 86% color accuracy in one epoch and highlights that deeper heads (depth 4–5) plus moderate dropout perform best with stronger shifts.

Next steps
- Re-run validation directly through the CLI (or a held-out test script) to confirm the true accuracy for the px3 models.
- Extend the best px2 and px3 sweep candidates to longer runs (30–100 epochs) while keeping early stopping identical for apples-to-apples comparisons.
- Compare the shift radii side-by-side and decide whether the extra preprocessing helps enough to keep the added complexity.

Experiment 6 (LAB color-space sweep)

Hypothesis
Switching the preprocessing pipeline from RGB to CIE LAB could expose luminance and opponent color signals more clearly to the classifier head, helping us recover 94%+ validation accuracy without extra edge tricks.

Steps
- Added a `--color-space` option to [src/hypertuning.py](src/hypertuning.py) so sweeps and single runs can choose between RGB and LAB workflows.
- Implemented an in-pipeline RGB→LAB tensor transform with channel-wise normalization, keeping edge-shift and evaluation hooks intact.
- Ran a 15-trial color-only sweep at 30 epochs per trial with `--color-space lab`; results and logs live in [Les1_modellogs/hyper_sweep_lab](Les1_modellogs/hyper_sweep_lab) with the aggregate metrics captured in [Les1_modellogs/hyper_sweep_lab/sweep_results.csv](Les1_modellogs/hyper_sweep_lab/sweep_results.csv).
- Generated a visual sanity check for the conversion at [data/external/lab_channels_sample.png](data/external/lab_channels_sample.png) showing the RGB source alongside the L, a, and b channels.

Findings (2026-01-02)
- LAB sweeps proved stable: the top quartile clustered between 0.94 and 0.95 validation accuracy, notably higher than the recent edge-shift experiments.
- The best LAB trial (trial 08) recorded 0.9531 validation accuracy with lr≈5.4e-3, momentum≈0.55, weight decay≈1.1e-4, head depth 2, width factor ≈1.53, dropout ≈0.26, and two unfrozen residual blocks.
- Even the weakest LAB run cleared 0.8849, so the color-space shift alone outperforms the stronger edge-shift baselines we tested earlier this week.

Next steps
- Promote the best LAB configuration to a 100-epoch run with early stopping patience 15 for head-to-head comparison against the RGB baseline.
- Mirror that configuration in RGB mode (same hyperparameters, transforms, patience) to isolate the color-space effect without confounding factors.
- Update downstream notebooks once the long runs confirm whether LAB consistently closes the gap to 96%.

Long-run follow-up (2026-01-03)

Steps
- Reran the trial 08 hyperparameters for up to 100 epochs with patience 15 and delta 0.001, logging to [Les1_modellogs/selected/color_lab_trial08_long/20260103-1242](Les1_modellogs/selected/color_lab_trial08_long/20260103-1242) and saving the restored weights to [models/flowers_resnet18_color_lab_trial08_long.pt](models/flowers_resnet18_color_lab_trial08_long.pt).
- Worked around the PyTorch 2.6 weights-only loader change inside [src/hypertuning.py](src/hypertuning.py#L1-L35) so early stopping can reload the best checkpoint automatically.
- Repeated the CLI evaluation helper so the LAB long-run numbers align with prior experiments.

Findings
- Early stopping halted at epoch 28 with the best checkpoint coming from epoch 13 (validation loss 0.1627).
- Final validation accuracy registered at 0.9403 on the color view and 0.7628 on the grayscale proxy, trailing the RGB baseline (0.9560 color / 0.7821 grayscale) but comfortably ahead of the edge-shift color runs.
- The restored checkpoint behaves consistently when reloaded through the CLI path after the loader fix, confirming the run’s metrics.

Next steps
- Explore modest patience bumps or data augmentation tweaks (mixup, color jitter) to see if LAB can close the remaining 0.0157 gap to the RGB baseline without overfitting.
- Consider hybrid approaches: start in LAB for faster convergence, then fine-tune in RGB using the same head to verify whether the spaces complement each other.
- Refresh any downstream analysis notebooks once follow-up runs confirm whether LAB reaches or surpasses 95% validation accuracy.

Plain-language recap (photo-editing lens)

- Starting from the untouched ImageNet weights mirrors loading a RAW file with zero adjustments, so the network barely guessed right (~15%).
- Converting to full grayscale was like focusing on luminance previews: short training pushed accuracy to roughly 82% while still coping with color scenes once flattened to gray.
- Rapid three-epoch sweeps played the role of exposure bracketing. They identified which RGB and grayscale settings were worth long runs, with the top RGB recipe near 90% and the best grayscale near 82%.
- A 100-epoch RGB fine-tune with early stopping delivered the polished “master print,” landing near 94% accuracy and holding up even after desaturating inputs.
- Edge-shift preprocessing behaved like strong sharpening filters—fun to experiment with, but it fell short of the RGB master and even degraded pure grayscale results.
- Switching to LAB separated lightness from color casts the way pro editors do. Sweeps clustered around 95%, and long runs stabilized near 94%, just a hair under the RGB master while aligning with photo-editing intuition.
- Net effect: photo-editing thinking steered us toward LAB as the most reliable gain, and the experiments overall mapped which stylistic tweaks actually move the validation accuracy dial.

Plain-language recap (photo-editing lens)

- Beginning with the untouched ImageNet weights was like opening a RAW file with no edits—the network guessed right only ~15% of the time.
- Converting everything to true grayscale let us focus on brightness detail the way a monochrome edit would, pushing accuracy to roughly 82% after a short tune while still coping with color scenes once they were flattened to gray.
- Quick three-epoch sweeps acted as exposure bracketing: they told us which RGB and grayscale settings were worth longer runs, with the top RGB recipe near 90% accuracy and the best grayscale near 82%.
- A 100-epoch RGB fine-tune with early stopping became our retouched “master print,” landing at ~94% accuracy and holding up even when images were desaturated.
- Edge-shift preprocessing behaved like aggressive sharpening—useful for experiments but it never beat the polished RGB baseline and actually hurt pure grayscale models, so we treat it as an optional creative filter.
- Switching to LAB separated lightness from color casts just like pro photo software, delivering the most consistent gains: sweeps hovered around 95%, and long runs settled near 94%, only a small step behind the RGB master while matching our color-artist intuition.
- Overall the photo-editing mindset helped us zero in on LAB preprocessing as the best lever, while the sharpening experiments provided context and confirmed where extra complexity did or did not pay off.
